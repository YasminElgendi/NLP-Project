{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add all imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from utils import *\n",
    "import re\n",
    "from pyarabic.araby import strip_diacritics\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import nltk\n",
    "import torch\n",
    "from torch import lstm_cell, nn\n",
    "import time\n",
    "import random\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, LSTM, Bidirectional, TimeDistributed , Input\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import glorot_normal\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################################################\n",
    "# Read the letters from the pickle files which we will use\n",
    "def get_letters():\n",
    "    file_path = 'constants/arabic_letters.pickle'\n",
    "    with open(file_path, 'rb') as file:\n",
    "        letters = pickle.load(file)\n",
    "    letters.add(\"<s>\")\n",
    "    letters.add(\"</s>\")\n",
    "    letters.add(\"<PAD>\")\n",
    "    return letters\n",
    "########################################################################################\n",
    "# Read the diacritics from the pickle files which we will use\n",
    "def get_diacritics():\n",
    "    file_path = 'constants/diacritics.pickle'\n",
    "    with open(file_path, 'rb') as file:\n",
    "        diacritics = pickle.load(file)\n",
    "    # diacritics.add(\"<s>\")\n",
    "    # diacritics.add(\"</s>\")\n",
    "    # diacritics.add(\"<PAD>\")\n",
    "    return diacritics\n",
    "########################################################################################\n",
    "# Read the diacritics from the pickle files which we will use\n",
    "def get_diacritics2id():\n",
    "\n",
    "    file_path = 'constants/diacritic2id.pickle'\n",
    "    with open(file_path, 'rb') as file:\n",
    "        diacritics2id = pickle.load(file)\n",
    "    # add no tashkeel\n",
    "    return diacritics2id\n",
    "\n",
    "########################################################################################\n",
    "# Read TRAINING dataset given\n",
    "def read_training_dataset(file_path = \"dataset/train.txt\"):\n",
    "    training_sentences = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        # Read each line in the file\n",
    "        for line in file:\n",
    "            # Strip any leading or trailing whitespace from the line\n",
    "            line = line.strip()\n",
    "            # Add the line to the list\n",
    "            training_sentences.append(line)\n",
    "    # if(len(training_sentences)==50000):\n",
    "    #     print(\"Read training set successfully\")\n",
    "    return training_sentences\n",
    "\n",
    "########################################################################################\n",
    "# Read DEV dataset given\n",
    "def read_dev_dataset(file_path = \"dataset/val.txt\"):\n",
    "    dev = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        # Read each line in the file\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            dev.append(line)\n",
    "    #print(len(dev))\n",
    "    # if(len(dev)==2500):\n",
    "    #     print(\"Read validation set successfully\")\n",
    "    return dev\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "def separate_word_to_letters_diacritics(arabic_text, arabic_letters=get_letters()):\n",
    "    # Normalize the text to handle different Unicode representations\n",
    "    normalized_text = unicodedata.normalize('NFKD', arabic_text)\n",
    "    letters = []\n",
    "    diacritics = []\n",
    "    # arabic_text = arabic_text[::-1]\n",
    "    # for ind in range(len(arabic_text)):\n",
    "    #     print(arabic_text[ind])\n",
    "    ind=0\n",
    "\n",
    "    while ind < len(arabic_text):\n",
    "        temp=[]\n",
    "        if not unicodedata.combining(arabic_text[ind]):\n",
    "            # print(arabic_text[ind])\n",
    "        # if arabic_text[ind] in arabic_letters:\n",
    "            letters.append(arabic_text[ind])\n",
    "            # print(\"added to letters\",arabic_text[ind])\n",
    "\n",
    "            if(ind+1 < len(arabic_text) and not unicodedata.combining(arabic_text[ind+1])):\n",
    "                diacritics.append(temp)\n",
    "                # print(\"added to diacritics from 1st\",temp)\n",
    "            if(ind == (len(arabic_text)-1)):\n",
    "              diacritics.append(temp)\n",
    "            ind+=1\n",
    "\n",
    "        else:\n",
    "            while ind < len(arabic_text) and unicodedata.combining(arabic_text[ind]):\n",
    "                # diacritics.pop(0)\n",
    "                # print(arabic_text[ind])\n",
    "                temp.append(arabic_text[ind])\n",
    "                ind+=1\n",
    "            temp=unicodedata.normalize('NFC', ''.join(temp))\n",
    "            # temp=[temp[::-1]]\n",
    "            diacritics.append([temp])\n",
    "            # print(\"added to diacritics\",temp)\n",
    "    # letters.reverse()\n",
    "    # diacritics.reverse()\n",
    "    return letters, diacritics\n",
    "\n",
    "########################################################################################\n",
    "def tokenize_to_vocab(data, vocab):\n",
    "    tokenized_sentences_word, tokenized_sentences_letters, tokenized_sentences_diacritics = [], [],[]\n",
    "\n",
    "    for d in (data):\n",
    "            tokens = nltk.word_tokenize(d, language=\"arabic\", preserve_line=True)\n",
    "            # Add the start sentence <s> and end sentence </s> tokens at the beginning and end of each tokenized sentence\n",
    "            tokens.reverse()\n",
    "            tokens.insert(0,\"<s>\")\n",
    "            tokens.append(\"</s>\")\n",
    "\n",
    "            vocab.update(tokens)\n",
    "\n",
    "            word_letters=[]\n",
    "            word_diacritics=[]\n",
    "            for token in (tokens):\n",
    "                if token != \"<s>\" and token != \"</s>\":\n",
    "                    # letters = separate_arabic_to_letters(token)\n",
    "                    letter, diacritic = separate_word_to_letters_diacritics(token)\n",
    "                    word_diacritics.append(diacritic)\n",
    "                    word_letters.append(letter)\n",
    "                else:\n",
    "                    word_letters.append(token)\n",
    "                    word_diacritics.append(token)\n",
    "\n",
    "            tokenized_sentences_letters.append(word_letters)\n",
    "            tokenized_sentences_diacritics.append(word_diacritics)\n",
    "            tokenized_sentences_word.append(tokens)\n",
    "\n",
    "    return vocab, tokenized_sentences_word,tokenized_sentences_letters,tokenized_sentences_diacritics\n",
    "\n",
    "def extract_sentences(training_dataset):\n",
    "    # This pattern keeps Arabic letters, diacritics, and whitespaces and endlines\n",
    "    pattern = re.compile(r'[^\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF\\s,.؟،«»؛(){};:!?\\-\\'\"]')\n",
    "\n",
    "    # Replace unmatched characters with an empty string\n",
    "    cleaned_corpus = [re.sub(pattern, \"\", t) for t in training_dataset]\n",
    "    cleaned_corpus = [re.sub(\"\\s\\s+\", \" \", c) for c in cleaned_corpus]\n",
    "\n",
    "    print(len(cleaned_corpus))\n",
    "\n",
    "    data,labels = [],[]\n",
    "\n",
    "    first = True\n",
    "    for c in cleaned_corpus:\n",
    "        sentences = re.split(r'[,.؟،«»؛(){};:!?\\-\\'\"]+', c)  # split on all punctuation\n",
    "        labels += sentences\n",
    "\n",
    "        without_dialects = [\n",
    "            strip_diacritics(s) for s in sentences\n",
    "        ]  # get the letters without dialects\n",
    "        data += without_dialects\n",
    "\n",
    "\n",
    "    # remove any spaces from line\n",
    "    data = [d.strip() for d in data]\n",
    "    labels = [l.strip() for l in labels]\n",
    "\n",
    "    # remove empty lines\n",
    "    data = [i for i in data if i]\n",
    "    labels = [i for i in labels if i]\n",
    "    return data,labels,cleaned_corpus\n",
    "\n",
    "\n",
    "########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 َ\n",
      "1 ً\n",
      "2 ُ\n",
      "3 ٌ\n",
      "4 ِ\n",
      "5 ٍ\n",
      "6 ْ\n",
      "7 ّ\n",
      "8 َّ\n",
      "9 ًّ\n",
      "10 ُّ\n",
      "11 ٌّ\n",
      "12 ِّ\n",
      "13 ٍّ\n",
      "14 \n",
      "15 <PAD>\n",
      "16 <s>\n",
      "17 </s>\n"
     ]
    }
   ],
   "source": [
    "#1- Read data embeddings we have for letters and diacritics\n",
    "letters, diacritics, diacritics2id = get_letters() , get_diacritics(), get_diacritics2id()\n",
    "#2- Have mapping ready\n",
    "#### Letters ---- IDs\n",
    "letters2id = {item: index for index, item in enumerate(letters)}\n",
    "letters2id[' ']=len(letters2id)-1\n",
    "id2letters = {index: item for index, item in enumerate(letters)}\n",
    "\n",
    "# add <PAD> to the mapping\n",
    "diacritics2id['<PAD>'] = len(diacritics2id)\n",
    "diacritics2id['<s>'] = len(diacritics2id)\n",
    "diacritics2id['</s>'] = len(diacritics2id)\n",
    "id2diacritics = {value: key for key, value in diacritics2id.items()}\n",
    "\n",
    "\n",
    "for diacritic, id in diacritics2id.items():\n",
    "        if diacritic not in diacritics:\n",
    "          diacritics.add(diacritic)\n",
    "\n",
    "for letter, id in letters2id.items():\n",
    "        if letter not in letters:\n",
    "          letters.add(letter)\n",
    "\n",
    "for id, dic in id2diacritics.items():\n",
    "      print(id, dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tHE MAIN MODEL AND FEEDING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 115\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X, Y\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# print(map_data([\"قَوْلُهُ\"]))\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# print(id2diacritics[0])\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDataGenerator\u001b[39;00m(\u001b[43mSequence\u001b[49m):\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;124;03m''' Costumized data generator to input line batches into the model '''\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lines, batch_size,letters2id, diacritics2id):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sequence' is not defined"
     ]
    }
   ],
   "source": [
    "# letters, diacritics, diacritics2id\n",
    "def map_data(data_raw):\n",
    "    ''' Splists data lines into an array of charachers as integers and an array of discritics as one-hot-encodings '''\n",
    "\n",
    "    # initialize data and diacritics lists\n",
    "    X = list()\n",
    "    Y = list()\n",
    "\n",
    "    # loop on data lines\n",
    "    for line in data_raw:\n",
    "\n",
    "        lit,dicc=separate_word_to_letters_diacritics(line)\n",
    "        # print(len(lit),len(dicc))\n",
    "        # for each sub list in dicc , if the len of the list is greater than 1 then take the last element only\n",
    "        # if the size is zero assert that the size is zero\n",
    "        # for i in range(len(dicc)):\n",
    "        #     if len(dicc[i])>=1:\n",
    "        #         dicc[i]=dicc[i][-1]\n",
    "        #     elif len(dicc[i])==0:\n",
    "        #         dicc[i]='<s>'\n",
    "\n",
    "        # initialize line data and diacritics lists and add start of sentence character\n",
    "        x = [letters2id['<s>']]\n",
    "        y = [diacritics2id['<s>']]\n",
    "        # x=[]\n",
    "        # y=[]\n",
    "        for i in range(min(len(lit),len(dicc))):\n",
    "            if len(dicc[i])>=1:\n",
    "                dicc[i]=dicc[i][0]\n",
    "            elif len(dicc[i])==0:\n",
    "                dicc[i]=''\n",
    "            if dicc[i] not in diacritics2id:\n",
    "                dicc[i]=''\n",
    "\n",
    "            if lit[i] not in letters2id:\n",
    "                lit[i]='<PAD>'\n",
    "                dicc[i]='<PAD>'\n",
    "\n",
    "            x.append(letters2id[lit[i]])\n",
    "            y.append(diacritics2id[dicc[i]])\n",
    "\n",
    "        # assert characters list length equals diacritics list length\n",
    "        # assert(len(x) == len(y))\n",
    "\n",
    "        # append end of sentence character\n",
    "        x.append(letters2id['</s>'])\n",
    "        y.append(diacritics2id['</s>'])\n",
    "\n",
    "        # convert diacritics integers to one_hot_encodings\n",
    "        # print(y)\n",
    "        y = to_categorical(y, len(diacritics2id))\n",
    "\n",
    "        # append line's data and diacritics lists to total data and diacritics lists\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    # convert lists to numpy arrays\n",
    "    # X = np.asarray(X)\n",
    "    # Y = np.asarray(Y)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def map_data(data_raw):\n",
    "    ''' Splists data lines into an array of charachers as integers and an array of discritics as one-hot-encodings '''\n",
    "\n",
    "    # initialize data and diacritics lists\n",
    "    X = list()\n",
    "    Y = list()\n",
    "\n",
    "    # loop on data lines\n",
    "    for line in data_raw:\n",
    "\n",
    "        lit,dicc=separate_word_to_letters_diacritics(line)\n",
    "\n",
    "        # initialize line data and diacritics lists and add start of sentence character\n",
    "        x = [letters2id['<s>']]\n",
    "        y = [diacritics2id['<s>']]\n",
    "        for idx, char in enumerate(line):\n",
    "            # skip character if it is only a dicritic\n",
    "            if char in diacritics:\n",
    "                continue\n",
    "            # append character mapping to data list\n",
    "            # if char not in letters:\n",
    "            #     continue\n",
    "            x.append(letters2id[char])\n",
    "            \n",
    "            # if character is not an arabic letter append whitespace to diacritics list\n",
    "            if char not in letters:\n",
    "                y.append(diacritics2id[''])\n",
    "            # if character is an arabic letter append its discritics (following 1 or 2 characters) to diacritics list\n",
    "            else:\n",
    "                char_diac = ''\n",
    "                if idx + 1 < len(line) and line[idx + 1] in diacritics:\n",
    "                    char_diac = line[idx + 1]\n",
    "                    if idx + 2 < len(line) and line[idx + 2] in diacritics and char_diac + line[idx + 2] in diacritics2id:\n",
    "                        char_diac += line[idx + 2]\n",
    "                    elif idx + 2 < len(line) and line[idx + 2] in diacritics and line[idx + 2] + char_diac in diacritics2id:\n",
    "                        char_diac = line[idx + 2] + char_diac\n",
    "                y.append(diacritics2id[char_diac])\n",
    "        \n",
    "        # append end of sentence character\n",
    "        x.append(letters2id['</s>'])\n",
    "        y.append(diacritics2id['</s>'])\n",
    "\n",
    "        # convert diacritics integers to one_hot_encodings\n",
    "        y = to_categorical(y, len(diacritics2id))\n",
    "\n",
    "        # append line's data and diacritics lists to total data and diacritics lists\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    # convert lists to numpy arrays\n",
    "    return X, Y\n",
    "# print(map_data([\"قَوْلُهُ\"]))\n",
    "# print(id2diacritics[0])\n",
    "class DataGenerator(Sequence):\n",
    "    ''' Costumized data generator to input line batches into the model '''\n",
    "    def __init__(self, lines, batch_size,letters2id, diacritics2id):\n",
    "        self.lines = lines\n",
    "        self.batch_size = batch_size\n",
    "        self.letters2id = letters2id\n",
    "        self.diacritics2id = diacritics2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.lines) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lines = self.lines[idx * self.batch_size:(idx + 1) * self.batch_size] # load number of sentences equal to batch size\n",
    "        X_batch, Y_batch = map_data(lines) # map data to integers and one-hot-encodings\n",
    "        X_max_seq_len = np.max([len(x) for x in X_batch])\n",
    "        Y_max_seq_len = np.max([len(y) for y in Y_batch])\n",
    "        assert(X_max_seq_len == Y_max_seq_len)\n",
    "        X = list()\n",
    "        for x in X_batch:\n",
    "            x = list(x)\n",
    "            x.extend([self.letters2id['<PAD>']] * (X_max_seq_len - len(x)))\n",
    "            X.append(np.asarray(x))\n",
    "\n",
    "        Y_tmp = list()\n",
    "        for y in Y_batch:\n",
    "            y_new = list(y)\n",
    "            y_new.extend(to_categorical([self.diacritics2id['<PAD>']] * (Y_max_seq_len - len(y)), len(diacritics2id)))\n",
    "            Y_tmp.append(np.asarray(y_new))\n",
    "        Y_batch = Y_tmp\n",
    "        X_batch = np.array(X)\n",
    "        Y_batch = np.asarray(Y_batch)\n",
    "\n",
    "        return (X_batch, Y_batch)\n",
    "\n",
    "########################################################################################\n",
    "def create_model(CHARACTERS_MAPPING, CLASSES_MAPPING ):\n",
    "    ''' Creates diacritization model '''\n",
    "    SelectedLSTM = LSTM # Bidirectional Long Short-Term Memory\n",
    "\n",
    "    inputs = Input(shape=(None,))\n",
    "\n",
    "    embeddings = Embedding(input_dim=len(CHARACTERS_MAPPING),\n",
    "                           output_dim=25,\n",
    "                           embeddings_initializer=glorot_normal(seed=961))(inputs)\n",
    "\n",
    "    blstm1 = Bidirectional(SelectedLSTM(units=256,\n",
    "                                     return_sequences=True,\n",
    "                                     kernel_initializer=glorot_normal(seed=961)))(embeddings)\n",
    "    dropout1 = Dropout(0.5)(blstm1)\n",
    "    blstm2 = Bidirectional(SelectedLSTM(units=256,\n",
    "                                     return_sequences=True,\n",
    "                                     kernel_initializer=glorot_normal(seed=961)))(dropout1)\n",
    "    dropout2 = Dropout(0.5)(blstm2)\n",
    "\n",
    "    dense1 = TimeDistributed(Dense(units=512,\n",
    "                                   activation='relu',\n",
    "                                   kernel_initializer=glorot_normal(seed=961)))(dropout2)\n",
    "    dense2 = TimeDistributed(Dense(units=512,\n",
    "                                   activation='relu',\n",
    "                                   kernel_initializer=glorot_normal(seed=961)))(dense1)\n",
    "\n",
    "    output = TimeDistributed(Dense(units=len(CLASSES_MAPPING),\n",
    "                                   activation='softmax',\n",
    "                                   kernel_initializer=glorot_normal(seed=961)))(dense2)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "\n",
    "    # compile model\n",
    "    # model.compile(loss='categorical_crossentropy', optimizer=Adam())\n",
    "    # use this model to calc the accuracy\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "########################################################################################\n",
    "\n",
    "def fit_model(model, epochs, batch_size, train_split,val_split,letters2id, diacritics2id):\n",
    "    ''' Fits model '''\n",
    "\n",
    "\n",
    "    # create training and validation generators\n",
    "    training_generator = DataGenerator(train_split, batch_size,letters2id, diacritics2id)\n",
    "    val_generator = DataGenerator(val_split, batch_size,letters2id, diacritics2id)\n",
    "\n",
    "    # fit model\n",
    "    history = model.fit(x=training_generator,\n",
    "              validation_data=val_generator,\n",
    "              epochs=epochs\n",
    "              )\n",
    "\n",
    "    # return history\n",
    "    return history\n",
    "########################################################################################\n",
    "def remove_diacritics(data_raw):\n",
    "  ''' Returns undiacritized text'''\n",
    "  return data_raw.translate(str.maketrans('', '', ''.join(diacritics)))\n",
    "def predict(line, model):\n",
    "    ''' predict test line '''\n",
    "    X, _ = map_data([line])\n",
    "    # for x in X[0]:\n",
    "    #   print(id2letters[x])\n",
    "    predictions = model.predict(X,verbose=0).squeeze()\n",
    "    # print(len(remove_diacritics(line)),len(predictions))\n",
    "\n",
    "    # print(predictions[0])\n",
    "    # # get most probable diacritizations for each character\n",
    "    predictions = predictions[1:]\n",
    "    # initialize empty output line\n",
    "    output = ''\n",
    "    char_list=[]\n",
    "    diac_list=[]\n",
    "    # loop on input characters and predicted diacritizations\n",
    "    for char, prediction in zip(remove_diacritics(line), predictions):\n",
    "        if char != ' ': # ignore spaces\n",
    "            char_list.append(char)\n",
    "            diac_list.append(np.argmax(prediction))\n",
    "    # ind=0\n",
    "    # for prediction in predictions:\n",
    "    #     char=X[ind]\n",
    "        # print(char)\n",
    "        # # ind+=1\n",
    "        # print(char,id2diacritics[np.argmax(prediction)])\n",
    "        # print(np.argmax(prediction),len(prediction),len(id2diacritics))\n",
    "\n",
    "        # append character\n",
    "        output += char\n",
    "        # if character is not an arabic letter continue\n",
    "        if char not in letters:\n",
    "            continue\n",
    "\n",
    "        if '<' in id2diacritics[np.argmax(prediction)]:\n",
    "            continue\n",
    "\n",
    "        # if character in arabic letters append predicted diacritization\n",
    "        output += id2diacritics[np.argmax(prediction)]\n",
    "\n",
    "    return output,char_list,diac_list\n",
    "\n",
    "print(id2diacritics[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = read_training_dataset()\n",
    "dev_dataset = read_dev_dataset()\n",
    "combined_dataset = training_dataset \n",
    "\n",
    "data, labels, cleaned_corpus = extract_sentences(combined_dataset)\n",
    "val_data, val_labels, val_cleaned_corpus = extract_sentences(dev_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- Clean the data\n",
    "\n",
    "# write the clean corpora to file\n",
    "# with open(\"./output_data/cleaned_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for l in cleaned_corpus:\n",
    "#         f.write(l + \"\\n\")\n",
    "\n",
    "# with open(\"./output_data/training_data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for d in data:\n",
    "#         f.write(str(d) + \"\\n\")\n",
    "\n",
    "# with open(\"./output_data/labeled/training_labels.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for l in labels:\n",
    "#         f.write(str(l) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2- Tokenize to vocab and words\n",
    "# vocab = set()\n",
    "# tokenized_data = []\n",
    "# vocab, tokenized_word_sentence, tokenized_letter_sentence, tokenized_diacritics_sentence = tokenize_to_vocab(labels, vocab)\n",
    "# stemmedVocab = []\n",
    "\n",
    "# Save the vocab to file each word in a line\n",
    "# with open(\"./output_data/vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for v in vocab:\n",
    "#         f.write(str(v) + \"\\n\")\n",
    "\n",
    "# # save the tokenized data sentence, as a form of list with <s> in index 0 and </s> in the last index\n",
    "#         # in between are words\n",
    "# with open(\"./output_data/tokenized_data_sentences.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for ts in tokenized_word_sentence:\n",
    "#         f.write(str(ts) + \"\\n\")\n",
    "\n",
    "# # save the tokenized data sentence, as a form of list with <s> in index 0 and </s> in the last index\n",
    "#         # in between are lists of letters\n",
    "# with open(\"./output_data/tokenized_data_letters.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for tl in tokenized_letter_sentence:\n",
    "#         f.write(str(tl) + \"\\n\")\n",
    "\n",
    "# # save the tokenized data sentence, as a form of list with <s> in index 0 and </s> in the last index\n",
    "#         # in between are lists of diacritics\n",
    "# with open(\"./output_data/tokenized_data_diacritics.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for td in tokenized_diacritics_sentence:\n",
    "#         f.write(str(td) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, None, 25)          975       \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirecti  (None, None, 512)         577536    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, None, 512)         0         \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirecti  (None, None, 512)         1574912   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, None, 512)         0         \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDi  (None, None, 512)         262656    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDi  (None, None, 512)         262656    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_8 (TimeDi  (None, None, 18)          9234      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2687969 (10.25 MB)\n",
      "Trainable params: 2687969 (10.25 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#1- We then pass the embeddings to an RNN Model\n",
    "# This model consists of an embedding layer, an RNN layer, and a fully connected layer. \n",
    "# The embedding layer transforms the input words (represented as integers) into dense vectors of fixed size. \n",
    "# The RNN layer processes these word embeddings sequentially, \n",
    "# maintaining an internal state that encodes information about the sequence so far. T\n",
    "# he fully connected layer transforms the output of the RNN layer to the desired output size.\n",
    "# with open( './constants/RNN_BIG_CHARACTERS_MAPPING.pickle', 'rb') as file:\n",
    "#     CHARACTERS_MAPPING = pkl.load(file)\n",
    "model = create_model(letters2id, diacritics2id)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FITING \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "hist_1 = fit_model(model, 10, 128, labels, val_labels,letters2id, diacritics2id)\n",
    "end_time = time.time()\n",
    "print('--- %s seconds ---' % round(end_time - start_time, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss history\n",
    "plt.plot(hist_1.history['loss'])\n",
    "plt.plot(hist_1.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(predict(\"العصفور فوق الشجرة\", model))\n",
    "test_data = read_dev_dataset('./dataset/test.txt')\n",
    "test_data,test_labels,test_corpus = extract_sentences(test_data)\n",
    "data_gen = DataGenerator(test_labels, 1,letters2id, diacritics2id)\n",
    "acc = model.evaluate(data_gen)[1]\n",
    "print(\"accracy in the test set is : \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict('ذهب زياد الي المدرسة', model)[0])\n",
    "print(predict(\"العصفور فوق الشجرة الكبيرة\", model)[0])\n",
    "print(predict(\"احمد يحب حنان\", model)[0])\n",
    "print(predict(\"الولد يلعب تحت الشجره\", model)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_epoch1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "new_model = load_model('model_epoch1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict('ذهب زياد الي المدرسة', model)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load test data and clean it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "ليس للوكيل بالقبض أن يبرأ المدين أو يهب الدين له أو يأخذ رهنا من المدين في مقابل الدين أو يقبل إحالته على شخص آخر لكن له أن يأخذ كفيلا لكن ليس له أن يأخذ كفيلا بشرط براءة الأصيل انظر المادة\n",
      "226\n",
      "--- 1.9 seconds ---\n",
      "182\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "# test_data = read_dev_dataset('./dataset/competition/test_no_diacritics.txt')\n",
    "test_data = read_dev_dataset('./test.txt')\n",
    "data, labels, cleaned_corpus = extract_sentences(test_data)\n",
    "\n",
    "last_id=0\n",
    "# open csv fiel as write\n",
    "file = open('result.csv', 'w', newline='', encoding='utf-8')\n",
    "writer = csv.writer(file)\n",
    "writer.writerow(['ID', 'label'])\n",
    "\n",
    "print(data[0])\n",
    "data = [\" \".join(data) ]\n",
    "print(len(data[0]))\n",
    "start_time = time.time()\n",
    "for i in range(len(data)):\n",
    "    out , char_list, diac_list = predict(data[i], model)\n",
    "    for j in range(len(char_list)):\n",
    "        if char_list[j] == ' ':\n",
    "            continue\n",
    "        # out_list.append([last_id, diac_list[j]])\n",
    "        writer.writerow([last_id, diac_list[j]])\n",
    "        last_id+=1\n",
    "\n",
    "file.close()\n",
    "\n",
    "end_time = time.time()\n",
    "print('--- %s seconds ---' % round(end_time - start_time, 2))\n",
    "print(last_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(last_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CALC THE ACCURACY IF WE HAVE THE GOLD OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.04\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "gold_data = pd.read_csv('sample_test_set_gold.csv')\n",
    "predicted_data = pd.read_csv('result.csv')  # Assuming this contains your predictions\n",
    "\n",
    "# Ensure both dataframes have the same length\n",
    "assert len(gold_data) == len(predicted_data), \"Datasets must have the same number of rows\"\n",
    "\n",
    "# Compare and calculate accuracy\n",
    "correct_predictions = 0\n",
    "for gold_label, predicted_label in zip(gold_data['label'], predicted_data['label']):\n",
    "    if gold_label == predicted_label:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / (len(gold_data)-1)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the data\n",
    "gold_data = pd.read_csv('./res/last_char_submission1.csv')\n",
    "predicted_data = pd.read_csv('./res/submission_7.csv')\n",
    "\n",
    "# Create a dictionary from gold_data for quick lookup\n",
    "gold_dict = dict(zip(gold_data['ID'], gold_data['label']))\n",
    "\n",
    "# Update predicted_data with labels from gold_data where IDs match\n",
    "for index, row in predicted_data.iterrows():\n",
    "    id = row['ID']\n",
    "    if id in gold_dict:\n",
    "        predicted_data.at[index, 'label'] = gold_dict[id]\n",
    "\n",
    "# Write the updated data to a new file\n",
    "predicted_data.to_csv('./res/updated_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0 4\n",
      "4 3 6\n",
      "6 6 4\n",
      "7 2 14\n",
      "8 2 4\n",
      "9 0 4\n",
      "10 0 14\n",
      "11 0 6\n",
      "13 8 6\n",
      "14 2 4\n",
      "15 4 0\n",
      "17 2 0\n",
      "18 2 6\n",
      "19 4 0\n",
      "20 6 0\n",
      "21 0 14\n",
      "22 0 6\n",
      "24 0 4\n",
      "25 4 14\n",
      "26 14 0\n",
      "27 1 0\n",
      "28 14 6\n",
      "29 2 0\n",
      "30 8 0\n",
      "32 4 14\n",
      "33 0 14\n",
      "34 0 8\n",
      "35 14 6\n",
      "36 2 0\n",
      "38 12 2\n",
      "39 2 0\n",
      "40 0 6\n",
      "41 8 0\n",
      "42 2 6\n",
      "43 0 2\n",
      "44 6 0\n",
      "45 3 0\n",
      "46 0 6\n",
      "47 6 1\n",
      "48 2 14\n",
      "49 0 4\n",
      "51 0 14\n",
      "52 0 6\n",
      "53 4 0\n",
      "54 14 4\n",
      "55 4 14\n",
      "56 0 4\n",
      "57 14 4\n",
      "58 0 14\n",
      "59 4 2\n",
      "61 6 14\n",
      "62 0 4\n",
      "64 6 14\n",
      "65 4 14\n",
      "66 0 8\n",
      "67 4 6\n",
      "68 13 4\n",
      "69 14 0\n",
      "71 0 2\n",
      "72 0 6\n",
      "73 14 0\n",
      "74 4 2\n",
      "75 0 14\n",
      "82 2 0\n",
      "83 3 14\n",
      "85 0 6\n",
      "86 0 5\n",
      "87 6 14\n",
      "88 4 0\n",
      "90 4 0\n",
      "91 14 4\n",
      "92 5 6\n",
      "94 14 2\n",
      "97 4 0\n",
      "98 14 6\n",
      "99 0 2\n",
      "100 6 0\n",
      "102 6 4\n",
      "103 3 14\n",
      "104 4 1\n",
      "105 0 14\n",
      "107 0 4\n",
      "108 14 6\n",
      "110 0 6\n",
      "111 6 0\n",
      "113 0 2\n",
      "114 8 0\n",
      "115 14 6\n",
      "116 4 0\n",
      "117 0 6\n",
      "118 14 2\n",
      "120 8 0\n",
      "121 0 4\n",
      "122 2 14\n",
      "123 0 1\n",
      "124 0 14\n",
      "125 6 4\n",
      "126 2 0\n",
      "127 0 6\n",
      "128 0 4\n",
      "129 14 0\n",
      "131 0 14\n",
      "133 14 4\n",
      "134 0 14\n",
      "136 14 0\n",
      "137 6 4\n",
      "138 5 14\n",
      "139 0 4\n",
      "140 0 2\n",
      "141 0 6\n",
      "142 0 2\n",
      "143 2 6\n",
      "144 0 14\n",
      "147 2 14\n",
      "148 6 8\n",
      "149 2 0\n",
      "153 14 6\n",
      "154 4 0\n",
      "155 4 6\n",
      "156 6 4\n",
      "157 4 10\n",
      "159 6 14\n",
      "160 2 8\n",
      "161 8 6\n",
      "162 4 0\n",
      "163 4 14\n",
      "164 0 4\n",
      "165 0 8\n",
      "166 2 0\n",
      "167 14 2\n",
      "168 2 8\n",
      "169 14 0\n",
      "171 10 6\n",
      "173 6 0\n",
      "174 2 14\n",
      "175 6 0\n",
      "176 2 14\n",
      "180 14 6\n",
      "181 4 0\n",
      "182 4 0\n",
      "183 0 6\n",
      "184 14 2\n",
      "185 6 2\n",
      "186 2 0\n",
      "188 6 0\n",
      "189 4 2\n",
      "190 6 4\n",
      "191 2 14\n",
      "192 14 0\n",
      "194 0 4\n",
      "196 4 14\n",
      "197 4 10\n",
      "199 0 4\n",
      "200 0 4\n",
      "201 8 0\n",
      "202 2 6\n",
      "203 6 0\n",
      "204 2 0\n",
      "205 2 5\n",
      "207 14 2\n",
      "209 0 8\n",
      "210 6 5\n",
      "211 4 0\n",
      "212 0 6\n",
      "213 8 0\n",
      "214 14 6\n",
      "215 4 0\n",
      "216 6 14\n",
      "218 14 0\n",
      "221 8 14\n",
      "223 4 0\n",
      "224 0 6\n",
      "225 2 0\n",
      "226 0 6\n",
      "227 6 4\n",
      "229 2 0\n",
      "231 2 0\n",
      "233 6 4\n",
      "234 5 0\n",
      "235 0 6\n",
      "236 0 5\n",
      "237 6 0\n",
      "238 2 4\n",
      "241 0 8\n",
      "242 8 5\n",
      "243 14 0\n",
      "244 14 0\n",
      "245 8 6\n",
      "246 0 2\n",
      "247 10 2\n",
      "248 2 0\n",
      "252 14 0\n",
      "254 14 0\n",
      "255 6 4\n",
      "257 6 4\n",
      "258 4 14\n",
      "259 4 5\n",
      "260 14 0\n",
      "261 4 0\n",
      "262 0 8\n",
      "263 8 0\n",
      "268 6 12\n",
      "269 4 8\n",
      "270 2 4\n",
      "271 6 4\n",
      "272 5 6\n",
      "273 0 4\n",
      "274 6 0\n",
      "275 2 4\n",
      "277 0 8\n",
      "278 8 0\n",
      "279 0 4\n",
      "282 0 6\n",
      "283 6 0\n",
      "284 4 6\n",
      "285 14 0\n",
      "286 4 0\n",
      "287 4 0\n",
      "288 0 4\n",
      "289 6 14\n",
      "291 1 14\n",
      "292 14 0\n",
      "293 0 11\n",
      "295 14 0\n",
      "297 8 14\n",
      "298 4 14\n",
      "299 14 6\n",
      "300 14 2\n",
      "301 8 6\n",
      "302 0 4\n",
      "303 10 14\n",
      "304 4 8\n",
      "305 0 4\n",
      "306 0 14\n",
      "308 14 0\n",
      "309 0 14\n",
      "310 2 0\n",
      "312 2 0\n",
      "313 0 6\n",
      "314 6 4\n",
      "315 4 14\n",
      "317 4 2\n",
      "319 6 12\n",
      "320 4 2\n",
      "321 0 4\n",
      "323 14 6\n",
      "324 0 4\n",
      "325 6 4\n",
      "326 4 0\n",
      "327 0 4\n",
      "328 0 6\n",
      "330 0 6\n",
      "331 6 0\n",
      "332 4 14\n",
      "333 0 4\n",
      "334 6 0\n",
      "335 4 6\n",
      "336 14 0\n",
      "337 6 0\n",
      "338 4 5\n",
      "339 14 2\n",
      "340 6 0\n",
      "341 0 8\n",
      "342 4 5\n",
      "343 4 0\n",
      "345 0 6\n",
      "346 14 2\n",
      "347 0 2\n",
      "349 0 14\n",
      "350 14 4\n",
      "351 2 0\n",
      "352 0 14\n",
      "353 2 0\n",
      "354 2 8\n",
      "356 6 0\n",
      "357 2 0\n",
      "358 14 4\n",
      "359 6 14\n",
      "360 4 14\n",
      "361 2 12\n",
      "362 7 8\n",
      "363 0 4\n",
      "364 6 4\n",
      "366 0 6\n",
      "367 0 4\n",
      "368 0 4\n",
      "369 14 4\n",
      "371 0 14\n",
      "372 6 0\n",
      "373 2 4\n",
      "374 2 6\n",
      "375 0 14\n",
      "376 6 4\n",
      "377 0 6\n",
      "379 14 6\n",
      "380 8 4\n",
      "384 0 14\n",
      "386 0 2\n",
      "387 0 6\n",
      "390 14 6\n",
      "391 14 0\n",
      "392 8 14\n",
      "393 0 6\n",
      "394 10 4\n",
      "395 4 14\n",
      "397 0 14\n",
      "398 14 4\n",
      "399 2 4\n",
      "400 4 0\n",
      "401 0 6\n",
      "402 4 0\n",
      "403 6 0\n",
      "404 0 4\n",
      "405 0 14\n",
      "406 14 6\n",
      "407 4 2\n",
      "408 4 0\n",
      "409 4 8\n",
      "410 14 4\n",
      "412 14 0\n",
      "413 0 6\n",
      "414 8 4\n",
      "415 0 14\n",
      "418 2 0\n",
      "419 0 8\n",
      "420 2 4\n",
      "422 8 0\n",
      "423 2 0\n",
      "425 0 14\n",
      "426 0 14\n",
      "427 14 8\n",
      "430 0 14\n",
      "431 14 2\n",
      "433 4 2\n",
      "436 14 8\n",
      "437 0 2\n",
      "438 14 0\n",
      "439 2 6\n",
      "440 2 4\n",
      "441 14 6\n",
      "442 4 0\n",
      "443 8 6\n",
      "446 2 4\n",
      "447 4 14\n",
      "448 0 6\n",
      "449 0 2\n",
      "450 14 0\n",
      "451 14 8\n",
      "452 6 4\n",
      "455 4 2\n",
      "456 0 2\n",
      "457 4 0\n",
      "460 6 14\n",
      "461 0 4\n",
      "462 0 4\n",
      "463 2 0\n",
      "464 0 6\n",
      "465 0 2\n",
      "466 14 0\n",
      "467 2 6\n",
      "468 2 0\n",
      "470 6 2\n",
      "471 2 4\n",
      "472 2 0\n",
      "473 0 14\n",
      "474 0 2\n",
      "476 8 6\n",
      "477 2 0\n",
      "478 0 6\n",
      "479 6 0\n",
      "481 14 6\n",
      "482 2 14\n",
      "483 2 14\n",
      "484 0 8\n",
      "485 0 6\n",
      "486 14 4\n",
      "487 0 14\n",
      "488 6 4\n",
      "489 4 0\n",
      "490 14 0\n",
      "492 8 0\n",
      "493 14 0\n",
      "494 4 0\n",
      "496 12 0\n",
      "497 0 12\n",
      "498 0 13\n",
      "499 6 0\n",
      "500 2 14\n",
      "501 0 4\n",
      "502 14 5\n",
      "504 2 6\n",
      "505 2 4\n",
      "506 0 6\n",
      "507 14 4\n",
      "508 0 14\n",
      "509 8 6\n",
      "511 2 6\n",
      "512 4 2\n",
      "513 6 14\n",
      "514 0 4\n",
      "516 0 8\n",
      "517 14 0\n",
      "518 4 0\n",
      "519 0 14\n",
      "520 4 2\n",
      "521 0 6\n",
      "522 4 0\n",
      "524 14 2\n",
      "526 0 2\n",
      "527 6 2\n",
      "528 4 0\n",
      "529 14 0\n",
      "530 14 0\n",
      "531 8 14\n",
      "532 14 0\n",
      "533 4 0\n",
      "534 14 8\n",
      "536 6 0\n",
      "537 1 14\n",
      "538 14 0\n",
      "539 0 14\n",
      "540 0 4\n",
      "541 14 3\n",
      "542 14 0\n",
      "546 6 10\n",
      "547 2 6\n",
      "548 6 4\n",
      "549 4 12\n",
      "550 4 0\n",
      "551 14 6\n",
      "552 14 2\n",
      "553 8 6\n",
      "554 2 0\n",
      "555 4 0\n",
      "556 4 0\n",
      "557 0 6\n",
      "558 8 0\n",
      "559 2 14\n",
      "560 0 4\n",
      "561 6 0\n",
      "563 2 0\n",
      "564 0 8\n",
      "565 6 0\n",
      "566 14 2\n",
      "567 6 14\n",
      "568 2 0\n",
      "569 6 14\n",
      "570 0 14\n",
      "571 14 8\n",
      "575 0 14\n",
      "577 0 14\n",
      "578 0 8\n",
      "579 10 2\n",
      "580 4 0\n",
      "581 14 0\n",
      "582 14 6\n",
      "583 8 4\n",
      "584 6 0\n",
      "585 4 0\n",
      "586 0 8\n",
      "587 6 0\n",
      "588 14 0\n",
      "589 6 14\n",
      "591 6 14\n",
      "592 0 6\n",
      "593 2 0\n",
      "594 4 0\n",
      "596 6 4\n",
      "597 4 6\n",
      "599 4 0\n",
      "601 8 4\n",
      "602 6 0\n",
      "603 0 4\n",
      "604 4 6\n",
      "605 4 0\n",
      "606 0 14\n",
      "607 8 4\n",
      "608 2 4\n",
      "609 0 14\n",
      "612 2 6\n",
      "613 0 2\n",
      "615 14 0\n",
      "616 0 8\n",
      "617 0 14\n",
      "618 2 6\n",
      "619 13 0\n",
      "620 2 6\n",
      "621 6 0\n",
      "622 0 4\n",
      "623 0 14\n",
      "624 5 0\n",
      "625 0 14\n",
      "627 0 14\n",
      "628 8 2\n",
      "629 14 4\n",
      "631 8 2\n",
      "632 14 2\n",
      "633 4 12\n",
      "634 0 8\n",
      "636 6 14\n",
      "637 0 6\n",
      "638 2 0\n",
      "639 14 0\n",
      "640 14 4\n",
      "642 7 4\n",
      "643 0 6\n",
      "644 14 0\n",
      "645 4 14\n",
      "646 14 0\n",
      "647 14 6\n",
      "648 8 14\n",
      "650 4 0\n",
      "652 14 2\n",
      "654 3 0\n",
      "655 4 14\n",
      "656 0 1\n",
      "657 6 14\n",
      "659 14 2\n",
      "660 2 4\n",
      "661 14 0\n",
      "662 8 6\n",
      "663 14 0\n",
      "665 6 8\n",
      "666 2 4\n",
      "668 14 6\n",
      "669 14 4\n",
      "670 14 0\n",
      "671 8 14\n",
      "672 6 4\n",
      "673 2 0\n",
      "674 0 12\n",
      "675 6 4\n",
      "676 0 4\n",
      "677 2 6\n",
      "679 0 14\n",
      "681 0 4\n",
      "682 0 6\n",
      "683 8 0\n",
      "684 4 14\n",
      "685 0 4\n",
      "686 6 4\n",
      "687 0 14\n",
      "688 14 6\n",
      "689 2 4\n",
      "691 6 14\n",
      "692 0 4\n",
      "693 6 4\n",
      "694 4 6\n",
      "695 4 0\n",
      "696 4 6\n",
      "697 0 4\n",
      "698 6 14\n",
      "699 5 6\n",
      "700 0 4\n",
      "701 0 6\n",
      "702 14 0\n",
      "703 0 14\n",
      "704 6 5\n",
      "705 0 4\n",
      "708 6 4\n",
      "709 4 14\n",
      "710 2 6\n",
      "711 8 0\n",
      "714 0 4\n",
      "715 2 6\n",
      "716 0 4\n",
      "717 6 0\n",
      "718 3 14\n",
      "719 0 4\n",
      "722 14 4\n",
      "723 6 14\n",
      "724 0 2\n",
      "725 14 2\n",
      "726 2 8\n",
      "727 2 0\n",
      "729 0 5\n",
      "730 6 0\n",
      "732 0 6\n",
      "733 6 0\n",
      "734 5 14\n",
      "736 0 6\n",
      "737 14 2\n",
      "740 14 0\n",
      "741 0 1\n",
      "742 2 0\n",
      "744 0 6\n",
      "745 14 0\n",
      "746 6 0\n",
      "747 0 2\n",
      "748 6 4\n",
      "749 0 14\n",
      "750 14 8\n",
      "751 0 6\n",
      "752 0 4\n",
      "753 8 4\n",
      "754 4 6\n",
      "757 4 6\n",
      "758 6 0\n",
      "760 6 4\n",
      "761 2 0\n",
      "762 0 6\n",
      "763 0 4\n",
      "764 14 4\n",
      "766 0 6\n",
      "767 4 2\n",
      "768 0 4\n",
      "769 14 0\n",
      "770 4 8\n",
      "771 4 14\n",
      "772 8 6\n",
      "775 4 14\n",
      "776 14 0\n",
      "778 14 2\n",
      "780 0 14\n",
      "781 8 6\n",
      "783 14 6\n",
      "784 6 0\n",
      "785 4 14\n",
      "786 0 2\n",
      "787 6 14\n",
      "788 4 14\n",
      "789 4 8\n",
      "790 0 12\n",
      "791 6 4\n",
      "793 4 2\n",
      "794 14 4\n",
      "795 4 14\n",
      "797 6 4\n",
      "798 2 5\n",
      "801 0 4\n",
      "802 5 3\n",
      "804 14 2\n",
      "805 6 2\n",
      "806 4 14\n",
      "808 0 4\n",
      "809 6 4\n",
      "810 0 14\n",
      "811 2 14\n",
      "812 6 8\n",
      "813 0 6\n",
      "814 6 4\n",
      "815 3 0\n",
      "816 4 6\n",
      "817 8 1\n",
      "821 8 4\n",
      "822 4 0\n",
      "823 4 0\n",
      "824 0 6\n",
      "826 6 0\n",
      "827 4 8\n",
      "829 6 14\n",
      "830 3 6\n",
      "832 6 4\n",
      "833 0 14\n",
      "834 14 2\n",
      "835 4 0\n",
      "836 6 8\n",
      "837 0 1\n",
      "839 6 0\n",
      "840 4 14\n",
      "841 0 6\n",
      "842 14 2\n",
      "843 4 0\n",
      "844 14 12\n",
      "845 14 2\n",
      "847 0 6\n",
      "848 6 0\n",
      "850 2 0\n",
      "852 6 14\n",
      "853 0 2\n",
      "854 2 6\n",
      "855 6 0\n",
      "857 6 2\n",
      "858 3 0\n",
      "860 6 14\n",
      "861 0 2\n",
      "862 2 14\n",
      "863 14 6\n",
      "864 14 2\n",
      "865 8 0\n",
      "866 6 12\n",
      "867 0 4\n",
      "868 0 4\n",
      "869 14 0\n",
      "870 6 8\n",
      "871 0 14\n",
      "875 2 14\n",
      "876 0 4\n",
      "877 12 0\n",
      "878 2 0\n",
      "879 14 6\n",
      "880 10 0\n",
      "881 2 0\n",
      "882 14 0\n",
      "883 2 6\n",
      "887 14 0\n",
      "888 6 10\n",
      "892 0 2\n",
      "893 14 0\n",
      "894 4 12\n",
      "896 2 4\n",
      "897 6 14\n",
      "898 0 6\n",
      "899 4 0\n",
      "900 1 0\n",
      "902 2 4\n",
      "903 6 14\n",
      "904 0 6\n",
      "905 4 0\n",
      "906 9 8\n",
      "907 14 4\n",
      "910 6 0\n",
      "911 0 14\n",
      "913 0 6\n",
      "914 2 0\n",
      "915 14 6\n",
      "917 2 6\n",
      "919 14 6\n",
      "920 0 14\n",
      "921 0 6\n",
      "922 0 4\n",
      "923 6 0\n",
      "924 0 14\n",
      "925 6 0\n",
      "926 0 6\n",
      "927 6 4\n",
      "929 6 14\n",
      "932 2 6\n",
      "933 0 2\n",
      "934 8 0\n",
      "935 2 8\n",
      "936 14 2\n",
      "938 14 6\n",
      "939 6 2\n",
      "940 4 0\n",
      "941 0 14\n",
      "942 14 2\n",
      "943 4 6\n",
      "944 4 2\n",
      "945 0 4\n",
      "946 14 0\n",
      "947 1 0\n",
      "948 14 0\n",
      "949 4 14\n",
      "950 0 2\n",
      "951 6 14\n",
      "952 0 6\n",
      "953 0 2\n",
      "954 14 0\n",
      "955 8 12\n",
      "956 14 4\n",
      "957 4 2\n",
      "958 14 0\n",
      "959 0 14\n",
      "961 4 2\n",
      "963 0 8\n",
      "964 14 2\n",
      "965 4 0\n",
      "966 0 14\n",
      "967 0 6\n",
      "968 2 0\n",
      "969 0 4\n",
      "971 4 2\n",
      "972 2 4\n",
      "975 2 0\n",
      "976 14 0\n",
      "977 6 14\n",
      "978 0 4\n",
      "979 14 0\n",
      "980 4 0\n",
      "981 4 14\n",
      "982 4 14\n",
      "983 14 6\n",
      "984 0 4\n",
      "985 14 6\n",
      "987 6 14\n",
      "988 0 4\n",
      "989 2 0\n",
      "990 0 2\n",
      "992 14 6\n",
      "996 14 8\n",
      "997 4 0\n",
      "998 0 4\n",
      "999 6 14\n",
      "784\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the data\n",
    "gold_data = pd.read_csv('./res/submission.csv')[:1000]\n",
    "predicted_data = pd.read_csv('./res/submission_7.csv')[:1000]\n",
    "\n",
    "miss=0\n",
    "correct_predictions=0\n",
    "ind=0\n",
    "for gold_label, predicted_label in zip(gold_data['label'], predicted_data['label']):\n",
    "    if gold_label == predicted_label:\n",
    "        correct_predictions += 1\n",
    "    else :\n",
    "        print(ind,gold_label,predicted_label)\n",
    "        miss+=1\n",
    "    ind+=1\n",
    "print(miss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
