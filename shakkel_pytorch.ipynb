{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add all imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import *\n",
    "import re\n",
    "from pyarabic.araby import strip_diacritics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\77\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import nltk\n",
    "import torch\n",
    "from torch import lstm_cell, nn\n",
    "import time\n",
    "import random\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    LSTM,\n",
    "    Bidirectional,\n",
    "    TimeDistributed,\n",
    "    Input,\n",
    ")\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import glorot_normal\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Read the letters from the pickle files which we will use\n",
    "def get_letters():\n",
    "    file_path = \"constants/arabic_letters.pickle\"\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        letters = pickle.load(file)\n",
    "    letters.add(\"<s>\")\n",
    "    letters.add(\"</s>\")\n",
    "    letters.add(\"<PAD>\")\n",
    "    return letters\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "# Read the diacritics from the pickle files which we will use\n",
    "def get_diacritics():\n",
    "    file_path = \"constants/diacritics.pickle\"\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        diacritics = pickle.load(file)\n",
    "    # diacritics.add(\"<s>\")\n",
    "    # diacritics.add(\"</s>\")\n",
    "    # diacritics.add(\"<PAD>\")\n",
    "    return diacritics\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "# Read the diacritics from the pickle files which we will use\n",
    "def get_diacritics2id():\n",
    "    file_path = \"constants/diacritic2id.pickle\"\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        diacritics2id = pickle.load(file)\n",
    "    # add no tashkeel\n",
    "    return diacritics2id\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "# Read TRAINING dataset given\n",
    "def read_training_dataset(file_path=\"dataset/train.txt\"):\n",
    "    training_sentences = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        # Read each line in the file\n",
    "        for line in file:\n",
    "            # Strip any leading or trailing whitespace from the line\n",
    "            line = line.strip()\n",
    "            # Add the line to the list\n",
    "            training_sentences.append(line)\n",
    "    # if(len(training_sentences)==50000):\n",
    "    #     print(\"Read training set successfully\")\n",
    "    return training_sentences\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "# Read DEV dataset given\n",
    "def read_dev_dataset(file_path=\"dataset/val.txt\"):\n",
    "    dev = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        # Read each line in the file\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            dev.append(line)\n",
    "    # print(len(dev))\n",
    "    # if(len(dev)==2500):\n",
    "    #     print(\"Read validation set successfully\")\n",
    "    return dev\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "def separate_word_to_letters_diacritics(arabic_text, arabic_letters=get_letters()):\n",
    "    # Normalize the text to handle different Unicode representations\n",
    "    normalized_text = unicodedata.normalize(\"NFKD\", arabic_text)\n",
    "    letters = []\n",
    "    diacritics = []\n",
    "    # arabic_text = arabic_text[::-1]\n",
    "    # for ind in range(len(arabic_text)):\n",
    "    #     print(arabic_text[ind])\n",
    "    ind = 0\n",
    "\n",
    "    while ind < len(arabic_text):\n",
    "        temp = []\n",
    "        if not unicodedata.combining(arabic_text[ind]):\n",
    "            # print(arabic_text[ind])\n",
    "            # if arabic_text[ind] in arabic_letters:\n",
    "            letters.append(arabic_text[ind])\n",
    "            # print(\"added to letters\",arabic_text[ind])\n",
    "\n",
    "            if ind + 1 < len(arabic_text) and not unicodedata.combining(\n",
    "                arabic_text[ind + 1]\n",
    "            ):\n",
    "                diacritics.append(temp)\n",
    "                # print(\"added to diacritics from 1st\",temp)\n",
    "            if ind == (len(arabic_text) - 1):\n",
    "                diacritics.append(temp)\n",
    "            ind += 1\n",
    "\n",
    "        else:\n",
    "            while ind < len(arabic_text) and unicodedata.combining(arabic_text[ind]):\n",
    "                # diacritics.pop(0)\n",
    "                # print(arabic_text[ind])\n",
    "                temp.append(arabic_text[ind])\n",
    "                ind += 1\n",
    "            temp = unicodedata.normalize(\"NFC\", \"\".join(temp))\n",
    "            # temp=[temp[::-1]]\n",
    "            diacritics.append([temp])\n",
    "            # print(\"added to diacritics\",temp)\n",
    "    # letters.reverse()\n",
    "    # diacritics.reverse()\n",
    "    return letters, diacritics\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "def tokenize_to_vocab(data, vocab):\n",
    "    (\n",
    "        tokenized_sentences_word,\n",
    "        tokenized_sentences_letters,\n",
    "        tokenized_sentences_diacritics,\n",
    "    ) = ([], [], [])\n",
    "\n",
    "    for d in data:\n",
    "        tokens = nltk.word_tokenize(d, language=\"arabic\", preserve_line=True)\n",
    "        # Add the start sentence <s> and end sentence </s> tokens at the beginning and end of each tokenized sentence\n",
    "        tokens.reverse()\n",
    "        tokens.insert(0, \"<s>\")\n",
    "        tokens.append(\"</s>\")\n",
    "\n",
    "        vocab.update(tokens)\n",
    "\n",
    "        word_letters = []\n",
    "        word_diacritics = []\n",
    "        for token in tokens:\n",
    "            if token != \"<s>\" and token != \"</s>\":\n",
    "                # letters = separate_arabic_to_letters(token)\n",
    "                letter, diacritic = separate_word_to_letters_diacritics(token)\n",
    "                word_diacritics.append(diacritic)\n",
    "                word_letters.append(letter)\n",
    "            else:\n",
    "                word_letters.append(token)\n",
    "                word_diacritics.append(token)\n",
    "\n",
    "        tokenized_sentences_letters.append(word_letters)\n",
    "        tokenized_sentences_diacritics.append(word_diacritics)\n",
    "        tokenized_sentences_word.append(tokens)\n",
    "\n",
    "    return (\n",
    "        vocab,\n",
    "        tokenized_sentences_word,\n",
    "        tokenized_sentences_letters,\n",
    "        tokenized_sentences_diacritics,\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_sentences(training_dataset):\n",
    "    # This pattern keeps Arabic letters, diacritics, and whitespaces and endlines\n",
    "    pattern = re.compile(\n",
    "        r'[^\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF\\s,.؟،«»؛(){};:!?\\-\\'\"]'\n",
    "    )\n",
    "\n",
    "    # Replace unmatched characters with an empty string\n",
    "    cleaned_corpus = [re.sub(pattern, \"\", t) for t in training_dataset]\n",
    "    cleaned_corpus = [re.sub(\"\\s\\s+\", \" \", c) for c in cleaned_corpus]\n",
    "\n",
    "    print(len(cleaned_corpus))\n",
    "\n",
    "    data, labels = [], []\n",
    "\n",
    "    first = True\n",
    "    for c in cleaned_corpus:\n",
    "        sentences = re.split(r'[,.؟،«»؛(){};:!?\\-\\'\"]+', c)  # split on all punctuation\n",
    "        labels += sentences\n",
    "\n",
    "        without_dialects = [\n",
    "            strip_diacritics(s) for s in sentences\n",
    "        ]  # get the letters without dialects\n",
    "        data += without_dialects\n",
    "\n",
    "    # remove any spaces from line\n",
    "    data = [d.strip() for d in data]\n",
    "    labels = [l.strip() for l in labels]\n",
    "\n",
    "    # remove empty lines\n",
    "    data = [i for i in data if i]\n",
    "    labels = [i for i in labels if i]\n",
    "    return data, labels, cleaned_corpus\n",
    "\n",
    "\n",
    "########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- Read data embeddings we have for letters and diacritics\n",
    "letters, diacritics, diacritics2id = (\n",
    "    get_letters(),\n",
    "    get_diacritics(),\n",
    "    get_diacritics2id(),\n",
    ")\n",
    "# 2- Have mapping ready\n",
    "#### Letters ---- IDs\n",
    "letters2id = {item: index for index, item in enumerate(letters)}\n",
    "id2letters = {index: item for index, item in enumerate(letters)}\n",
    "\n",
    "# add <PAD> to the mapping\n",
    "diacritics2id[\"<PAD>\"] = len(diacritics2id)\n",
    "diacritics2id[\"<s>\"] = len(diacritics2id)\n",
    "diacritics2id[\"</s>\"] = len(diacritics2id)\n",
    "id2diacritics = {value: key for key, value in diacritics2id.items()}\n",
    "for diacritic, id in diacritics2id.items():\n",
    "    if diacritic not in diacritics:\n",
    "        diacritics.add(diacritic)\n",
    "\n",
    "for letter, id in letters2id.items():\n",
    "    if letter not in letters:\n",
    "        letters.add(letter)\n",
    "# print(diacritics2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_data(data_raw):\n",
    "    \"\"\"Splists data lines into an array of charachers as integers and an array of discritics as one-hot-encodings\"\"\"\n",
    "\n",
    "    # initialize data and diacritics lists\n",
    "    X = list()\n",
    "    Y = list()\n",
    "\n",
    "    # loop on data lines\n",
    "    for line in data_raw:\n",
    "        lit, dicc = separate_word_to_letters_diacritics(line)\n",
    "        x = [letters2id[\"<s>\"]]\n",
    "        y = [diacritics2id[\"<s>\"]]\n",
    "        for i in range(min(len(lit), len(dicc))):\n",
    "            if len(dicc[i]) >= 1:\n",
    "                dicc[i] = dicc[i][0]\n",
    "            elif len(dicc[i]) == 0:\n",
    "                dicc[i] = \"\"\n",
    "            if dicc[i] not in diacritics2id:\n",
    "                dicc[i] = \"\"\n",
    "\n",
    "            if lit[i] not in letters2id:\n",
    "                lit[i] = \"<PAD>\"\n",
    "                dicc[i] = \"<PAD>\"\n",
    "\n",
    "            x.append(letters2id[lit[i]])\n",
    "            y.append(diacritics2id[dicc[i]])\n",
    "        # append end of sentence character\n",
    "        x.append(letters2id[\"</s>\"])\n",
    "        y.append(diacritics2id[\"</s>\"])\n",
    "\n",
    "        # convert diacritics integers to one_hot_encodings\n",
    "        # y = to_categorical(y, len(diacritics2id))\n",
    "\n",
    "        # append line's data and diacritics lists to total data and diacritics lists\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tHE MAIN MODEL AND FEEDING\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class DiacriticsDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, lines, letters2id, diacritics2id, device, max_length=256, padding_value=0\n",
    "    ):\n",
    "        self.lines = lines\n",
    "        X, Y = map_data(lines)\n",
    "        X = [torch.LongTensor(x) for x in X]\n",
    "        Y = [torch.LongTensor(x) for x in Y]\n",
    "        X_padded = pad_sequence(X, batch_first=True, padding_value=padding_value)[\n",
    "            :, :max_length\n",
    "        ]\n",
    "        Y_padded = pad_sequence(Y, batch_first=True, padding_value=padding_value)[\n",
    "            :, :max_length\n",
    "        ]\n",
    "        X_padded = X_padded.to(device)\n",
    "        Y_padded = Y_padded.to(device)\n",
    "        self.x = X_padded\n",
    "        self.y = Y_padded\n",
    "        self.letters2id = letters2id\n",
    "        self.diacritics2id = diacritics2id\n",
    "        self.max_length = max_length\n",
    "        self.padding_value = padding_value\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # line = self.lines[idx]\n",
    "        # X, Y = map_data([line])  # Your existing function\n",
    "        # X = torch.LongTensor(X[0])[:, : self.max_length]\n",
    "        # Y = torch.FloatTensor(Y[0])[;, :self]\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    # @staticmethod\n",
    "    # def collate_fn(batch, padding_value=0):\n",
    "    #     # print(self.max_length)\n",
    "    #     # while True:\n",
    "    #     #     pass\n",
    "    #     X_batch, Y_batch = zip(*batch)\n",
    "\n",
    "    #     # Pad X_batch\n",
    "    #     X_padded = pad_sequence(X_batch, batch_first=True, padding_value=padding_value)\n",
    "\n",
    "    #     # Pad Y_batch\n",
    "    #     # If Y_batch is 2-dimensional (e.g., just indices), it should be padded differently.\n",
    "    #     Y_padded = pad_sequence(Y_batch, batch_first=True, padding_value=padding_value)\n",
    "\n",
    "    #     # Ensure all sequences are the same length\n",
    "    #     # This is important if X and Y can be of different lengths; adjust as needed.\n",
    "    #     # max_length = max(X_padded.shape[1], Y_padded.shape[1])\n",
    "    #     max_length = 256\n",
    "    #     if X_padded.shape[1] < max_length:\n",
    "    #         # Pad X_padded to max_length\n",
    "    #         padding = X_padded.new_full(\n",
    "    #             (X_padded.size(0), max_length - X_padded.size(1), *X_padded.shape[2:]),\n",
    "    #             padding_value,\n",
    "    #         )\n",
    "    #         X_padded = torch.cat([X_padded, padding], dim=1)\n",
    "    #     if Y_padded.shape[1] < max_length:\n",
    "    #         # Pad Y_padded to max_length\n",
    "    #         padding = Y_padded.new_full(\n",
    "    #             (Y_padded.size(0), max_length - Y_padded.size(1)), padding_value\n",
    "    #         )\n",
    "    #         Y_padded = torch.cat([Y_padded, padding], dim=1)\n",
    "\n",
    "    #     return X_padded, Y_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "# class DiacritizationModel(nn.Module):\n",
    "#     def __init__(self, letters2id, diacritics2id):\n",
    "#         super(DiacritizationModel, self).__init__()\n",
    "#         self.embedding = nn.Embedding(num_embeddings=len(letters2id), embedding_dim=25)\n",
    "#         self.blstm1 = nn.LSTM(\n",
    "#             input_size=25, hidden_size=256, bidirectional=True, batch_first=True\n",
    "#         )\n",
    "#         self.dropout1 = nn.Dropout(0.1)\n",
    "#         self.blstm2 = nn.LSTM(\n",
    "#             input_size=512, hidden_size=256, bidirectional=True, batch_first=True\n",
    "#         )\n",
    "#         self.dropout2 = nn.Dropout(0.1)\n",
    "#         self.dense1 = nn.Linear(512, 512)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.dense2 = nn.Linear(512, len(diacritics2id))\n",
    "#         self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.embedding(x)\n",
    "#         x, _ = self.blstm1(x)\n",
    "#         x = self.dropout1(x)\n",
    "#         x, _ = self.blstm2(x)\n",
    "#         x = self.dropout2(x)\n",
    "#         x = self.dense1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.dense2(x)\n",
    "#         x = self.softmax(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "class DiacritizationModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        letters2id,\n",
    "        diacritics2id,\n",
    "        hidden_size=256,\n",
    "        embedding_dim=256,\n",
    "        in_vocab=25,\n",
    "        out_vocab=25,\n",
    "    ):\n",
    "        super(DiacritizationModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=len(letters2id), embedding_dim=embedding_dim\n",
    "        )\n",
    "\n",
    "        self.blstm1 = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "\n",
    "        self.blstm2 = nn.LSTM(\n",
    "            input_size=hidden_size * 2,\n",
    "            hidden_size=hidden_size,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.dense1 = nn.Linear(hidden_size * 2, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(512, len(diacritics2id))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.blstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.blstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "def train_model(model, epochs, train_loader, val_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters())\n",
    "    padding_value = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            # print(inputs.shape, targets.shape)\n",
    "\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)  #\n",
    "\n",
    "            loss = criterion(\n",
    "                outputs.transpose(1, 2), targets\n",
    "            )  # Assumes targets are one-hot encoded\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print statistics\n",
    "            if i % 100 == 0:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item()}\"\n",
    "                )\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.transpose(1, 2), targets)\n",
    "                total_loss += loss.item()\n",
    "            print(f\"Validation Loss: {total_loss / len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pytorch(line, model, letters2id, diacritics2id, id2diacritics):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    model.to(device)  # Ensure the model is on the correct device\n",
    "\n",
    "    # Preprocess the input line\n",
    "    # print(line)\n",
    "    X, _ = map_data([line])  # Use your existing function\n",
    "    X = (\n",
    "        torch.LongTensor(X[0]).unsqueeze(0).to(device)\n",
    "    )  # Add batch dimension and move to device\n",
    "\n",
    "    # Predict using the model\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X).squeeze(0)  # Remove batch dimension\n",
    "\n",
    "    # Move predictions to CPU for further processing if necessary\n",
    "    predictions = predictions.cpu()\n",
    "\n",
    "    # Process the model's output\n",
    "    output = \"\"\n",
    "    char_list = []\n",
    "    diac_list = []\n",
    "    undiacritized_line = line\n",
    "\n",
    "    # Map the predicted diacritics back to the original text\n",
    "    for char, pred in zip(undiacritized_line, predictions):\n",
    "        # if char != \" \":\n",
    "        char_list.append(char)\n",
    "        diacritic_index = (\n",
    "            pred.argmax().item()\n",
    "        )  # Get the index of the highest probability\n",
    "        diac_list.append(diacritic_index)\n",
    "\n",
    "        output += char\n",
    "        # Append the diacritic if it's a valid Arabic letter and has a corresponding diacritic\n",
    "        if char in letters and \"<\" not in id2diacritics[diacritic_index]:\n",
    "            output += id2diacritics[diacritic_index]\n",
    "\n",
    "    return output, char_list, diac_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = read_training_dataset()\n",
    "dev_dataset = read_dev_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process and clean data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "2500\n"
     ]
    }
   ],
   "source": [
    "# 1- Clean the data\n",
    "data, labels, cleaned_corpus = extract_sentences(training_dataset)\n",
    "val_data, val_labels, val_cleaned_corpus = extract_sentences(dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate the lengths of all strings\n",
    "lengths = np.array([len(s) for s in data])\n",
    "\n",
    "# Determine the length that keeps 99% of the data\n",
    "length_99_percentile = int(np.percentile(lengths, 99))\n",
    "\n",
    "# length_99_percentile = 500\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    DiacriticsDataset(\n",
    "        labels, letters2id, diacritics2id, device, max_length=length_99_percentile\n",
    "    ),\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    # collate_fn=DiacriticsDataset.collate_fn,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    DiacriticsDataset(\n",
    "        val_labels, letters2id, diacritics2id, device, max_length=length_99_percentile\n",
    "    ),\n",
    "    batch_size=128,\n",
    "    # collate_fn=DiacriticsDataset.collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [1/2175], Loss: 2.9391896724700928\n",
      "Epoch [1/3], Step [101/2175], Loss: 0.10735046863555908\n",
      "Epoch [1/3], Step [201/2175], Loss: 0.07242868095636368\n",
      "Epoch [1/3], Step [301/2175], Loss: 0.047480203211307526\n",
      "Epoch [1/3], Step [401/2175], Loss: 0.0545131154358387\n",
      "Epoch [1/3], Step [501/2175], Loss: 0.034626808017492294\n",
      "Epoch [1/3], Step [601/2175], Loss: 0.03932067006826401\n",
      "Epoch [1/3], Step [701/2175], Loss: 0.03423409163951874\n",
      "Epoch [1/3], Step [801/2175], Loss: 0.023445913568139076\n",
      "Epoch [1/3], Step [901/2175], Loss: 0.029815522953867912\n",
      "Epoch [1/3], Step [1001/2175], Loss: 0.02192300744354725\n",
      "Epoch [1/3], Step [1101/2175], Loss: 0.030136939138174057\n",
      "Epoch [1/3], Step [1201/2175], Loss: 0.03407389298081398\n",
      "Epoch [1/3], Step [1301/2175], Loss: 0.027252042666077614\n",
      "Epoch [1/3], Step [1401/2175], Loss: 0.022839564830064774\n",
      "Epoch [1/3], Step [1501/2175], Loss: 0.021497759968042374\n",
      "Epoch [1/3], Step [1601/2175], Loss: 0.02180376648902893\n",
      "Epoch [1/3], Step [1701/2175], Loss: 0.030350688844919205\n",
      "Epoch [1/3], Step [1801/2175], Loss: 0.02089248225092888\n",
      "Epoch [1/3], Step [1901/2175], Loss: 0.024733910337090492\n",
      "Epoch [1/3], Step [2001/2175], Loss: 0.01995876431465149\n",
      "Epoch [1/3], Step [2101/2175], Loss: 0.01855498179793358\n",
      "Validation Loss: 0.0175366880970874\n",
      "Epoch [2/3], Step [1/2175], Loss: 0.015491020865738392\n",
      "Epoch [2/3], Step [101/2175], Loss: 0.015447928570210934\n",
      "Epoch [2/3], Step [201/2175], Loss: 0.019161473959684372\n",
      "Epoch [2/3], Step [301/2175], Loss: 0.015032845549285412\n",
      "Epoch [2/3], Step [401/2175], Loss: 0.022271737456321716\n",
      "Epoch [2/3], Step [501/2175], Loss: 0.01985679566860199\n",
      "Epoch [2/3], Step [601/2175], Loss: 0.01802940107882023\n",
      "Epoch [2/3], Step [701/2175], Loss: 0.02150736004114151\n",
      "Epoch [2/3], Step [801/2175], Loss: 0.01771789789199829\n",
      "Epoch [2/3], Step [901/2175], Loss: 0.01982099376618862\n",
      "Epoch [2/3], Step [1001/2175], Loss: 0.014141245745122433\n",
      "Epoch [2/3], Step [1101/2175], Loss: 0.01587439328432083\n",
      "Epoch [2/3], Step [1201/2175], Loss: 0.021464457735419273\n",
      "Epoch [2/3], Step [1301/2175], Loss: 0.017151538282632828\n",
      "Epoch [2/3], Step [1401/2175], Loss: 0.01566186174750328\n",
      "Epoch [2/3], Step [1501/2175], Loss: 0.018152646720409393\n",
      "Epoch [2/3], Step [1601/2175], Loss: 0.01331763993948698\n",
      "Epoch [2/3], Step [1701/2175], Loss: 0.020759889855980873\n",
      "Epoch [2/3], Step [1801/2175], Loss: 0.01935494691133499\n",
      "Epoch [2/3], Step [1901/2175], Loss: 0.01730780117213726\n",
      "Epoch [2/3], Step [2001/2175], Loss: 0.018276941031217575\n",
      "Epoch [2/3], Step [2101/2175], Loss: 0.01254994049668312\n",
      "Validation Loss: 0.013566214695206977\n",
      "Epoch [3/3], Step [1/2175], Loss: 0.015211980789899826\n",
      "Epoch [3/3], Step [101/2175], Loss: 0.014972732402384281\n",
      "Epoch [3/3], Step [201/2175], Loss: 0.018811428919434547\n",
      "Epoch [3/3], Step [301/2175], Loss: 0.011798444204032421\n",
      "Epoch [3/3], Step [401/2175], Loss: 0.013893154449760914\n",
      "Epoch [3/3], Step [501/2175], Loss: 0.020186586305499077\n",
      "Epoch [3/3], Step [601/2175], Loss: 0.013329460285604\n",
      "Epoch [3/3], Step [701/2175], Loss: 0.013957653194665909\n",
      "Epoch [3/3], Step [801/2175], Loss: 0.018426965922117233\n",
      "Epoch [3/3], Step [901/2175], Loss: 0.014828719198703766\n",
      "Epoch [3/3], Step [1001/2175], Loss: 0.012690132483839989\n",
      "Epoch [3/3], Step [1101/2175], Loss: 0.012849644757807255\n",
      "Epoch [3/3], Step [1201/2175], Loss: 0.014848517253994942\n",
      "Epoch [3/3], Step [1301/2175], Loss: 0.014781404286623001\n",
      "Epoch [3/3], Step [1401/2175], Loss: 0.014282139018177986\n",
      "Epoch [3/3], Step [1501/2175], Loss: 0.012232819572091103\n",
      "Epoch [3/3], Step [1601/2175], Loss: 0.014351291581988335\n",
      "Epoch [3/3], Step [1701/2175], Loss: 0.009766737930476665\n",
      "Epoch [3/3], Step [1801/2175], Loss: 0.0138408737257123\n",
      "Epoch [3/3], Step [1901/2175], Loss: 0.012230497784912586\n",
      "Epoch [3/3], Step [2001/2175], Loss: 0.017596587538719177\n",
      "Epoch [3/3], Step [2101/2175], Loss: 0.014314883388578892\n",
      "Validation Loss: 0.012300549648768668\n"
     ]
    }
   ],
   "source": [
    "model = DiacritizationModel(letters2id, diacritics2id)\n",
    "\n",
    "# print(train_loader[0])\n",
    "lolo = train_model(model, 3, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './saved_models_pytorch/three_epochs_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "accracy in the test set is :  اُلُعُصُفُوُرُ فُوُقُ اُلُشَجُرُةُ\n"
     ]
    }
   ],
   "source": [
    "# print(predict(\"العصفور فوق الشجرة\", model))\n",
    "test_data = read_dev_dataset(\"dataset/val.txt\")\n",
    "test_data, test_labels, test_corpus = extract_sentences(test_data)\n",
    "# data_gen = DataGenerator(test_labels, 1,letters2id, diacritics2id)\n",
    "data_gen = DiacriticsDataset(\n",
    "    test_labels, letters2id, diacritics2id, device, max_length=length_99_percentile\n",
    ")\n",
    "# acc = model.evaluate(data_gen)[1]\n",
    "acc = predict_pytorch(\n",
    "    \"العصفور فوق الشجرة\", model, letters2id, diacritics2id, id2diacritics\n",
    ")\n",
    "print(\"accracy in the test set is : \", acc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(predict(\"ذهب زياد الي المدرسة\", model)[0])\n",
    "# print(predict(\"العصفور فوق الشجرة الكبيرة\", model)[0])\n",
    "# print(predict(\"احمد يحب حنان\", model)[0])\n",
    "# print(predict(\"الولد يلعب تحت الشجره\", model)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DiacritizationModel' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_epoch1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\77\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DiacritizationModel' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "model.save(\"model_epoch1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "new_model = load_model(\"model_epoch1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load test data and clean it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "test_data = read_dev_dataset(\"./test.txt\")\n",
    "data, labels, cleaned_corpus = extract_sentences(test_data)\n",
    "last_id = 0\n",
    "# open csv fiel as write\n",
    "file = open(\"result.csv\", \"w\", newline=\"\", encoding=\"utf-8\")\n",
    "writer = csv.writer(file)\n",
    "writer.writerow([\"ID\", \"label\"])\n",
    "\n",
    "\n",
    "for i in range(len(data)):\n",
    "    out, char_list, diac_list = predict(data[i], model)\n",
    "    for j in range(len(char_list)):\n",
    "        if char_list[j] == \" \":\n",
    "            continue\n",
    "        writer.writerow([last_id, diac_list[j]])\n",
    "        last_id += 1\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CALC THE ACCURACY IF WE HAVE THE GOLD OUTPUT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "gold_data = pd.read_csv(\"sample_test_set_gold.csv\")\n",
    "predicted_data = pd.read_csv(\"result.csv\")  # Assuming this contains your predictions\n",
    "\n",
    "# Ensure both dataframes have the same length\n",
    "assert len(gold_data) == len(\n",
    "    predicted_data\n",
    "), \"Datasets must have the same number of rows\"\n",
    "\n",
    "# Compare and calculate accuracy\n",
    "correct_predictions = 0\n",
    "for gold_label, predicted_label in zip(gold_data[\"label\"], predicted_data[\"label\"]):\n",
    "    if gold_label == predicted_label:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / (len(gold_data) - 1)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
