{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Z2gphlGoNrd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "606af6ce-d13c-49b0-e430-0295d7c64cc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement unicodedata (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for unicodedata\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q tensorflow\n",
        "!pip install -q PyArabic\n",
        "!pip install -q torch\n",
        "!pip install -q pickle\n",
        "!pip install -q unicodedata\n",
        "!pip install -q pyyaml h5py\n",
        "!pip install -q gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wdgrQcMOj_D",
        "outputId": "3a594bed-41ac-42e2-d360-fe2bf9cda742"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'NLP-Project'\n",
            "/content/NLP-Project\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://YasminElgendi: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@github.com/YasminElgendi/NLP-Project.git\n",
        "\n",
        "%cd NLP-Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xLJWWkpZOoUP"
      },
      "outputs": [],
      "source": [
        "# from utils import *\n",
        "import re\n",
        "from pyarabic.araby import strip_diacritics\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import nltk\n",
        "import torch\n",
        "from torch import lstm_cell, nn\n",
        "import time\n",
        "import random\n",
        "import pickle as pkl\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout, LSTM, Bidirectional, TimeDistributed , Input\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import glorot_normal\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7XmzDVMrOrtT"
      },
      "outputs": [],
      "source": [
        "\n",
        "########################################################################################\n",
        "# Read the letters from the pickle files which we will use\n",
        "def get_letters():\n",
        "    file_path = 'constants/arabic_letters.pickle'\n",
        "    with open(file_path, 'rb') as file:\n",
        "        letters = pickle.load(file)\n",
        "    letters.add(\"<s>\")\n",
        "    letters.add(\"</s>\")\n",
        "    letters.add(\"<PAD>\")\n",
        "    return letters\n",
        "########################################################################################\n",
        "# Read the diacritics from the pickle files which we will use\n",
        "def get_diacritics():\n",
        "    file_path = 'constants/diacritics.pickle'\n",
        "    with open(file_path, 'rb') as file:\n",
        "        diacritics = pickle.load(file)\n",
        "    # diacritics.add(\"<s>\")\n",
        "    # diacritics.add(\"</s>\")\n",
        "    # diacritics.add(\"<PAD>\")\n",
        "    return diacritics\n",
        "########################################################################################\n",
        "# Read the diacritics from the pickle files which we will use\n",
        "def get_diacritics2id():\n",
        "\n",
        "    file_path = 'constants/diacritic2id.pickle'\n",
        "    with open(file_path, 'rb') as file:\n",
        "        diacritics2id = pickle.load(file)\n",
        "    # add no tashkeel\n",
        "    return diacritics2id\n",
        "\n",
        "########################################################################################\n",
        "# Read TRAINING dataset given\n",
        "def read_training_dataset(file_path = \"dataset/train.txt\"):\n",
        "    training_sentences = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        # Read each line in the file\n",
        "        for line in file:\n",
        "            # Strip any leading or trailing whitespace from the line\n",
        "            line = line.strip()\n",
        "            # Add the line to the list\n",
        "            training_sentences.append(line)\n",
        "    # if(len(training_sentences)==50000):\n",
        "    #     print(\"Read training set successfully\")\n",
        "    return training_sentences\n",
        "\n",
        "########################################################################################\n",
        "# Read DEV dataset given\n",
        "def read_dev_dataset(file_path = \"dataset/val.txt\"):\n",
        "    dev = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        # Read each line in the file\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            dev.append(line)\n",
        "    #print(len(dev))\n",
        "    # if(len(dev)==2500):\n",
        "    #     print(\"Read validation set successfully\")\n",
        "    return dev\n",
        "\n",
        "########################################################################################\n",
        "\n",
        "def separate_word_to_letters_diacritics(arabic_text, arabic_letters=get_letters()):\n",
        "    # Normalize the text to handle different Unicode representations\n",
        "    normalized_text = unicodedata.normalize('NFKD', arabic_text)\n",
        "    letters = []\n",
        "    diacritics = []\n",
        "    # arabic_text = arabic_text[::-1]\n",
        "    # for ind in range(len(arabic_text)):\n",
        "    #     print(arabic_text[ind])\n",
        "    ind=0\n",
        "\n",
        "    while ind < len(arabic_text):\n",
        "        temp=[]\n",
        "        if not unicodedata.combining(arabic_text[ind]):\n",
        "            # print(arabic_text[ind])\n",
        "        # if arabic_text[ind] in arabic_letters:\n",
        "            letters.append(arabic_text[ind])\n",
        "            # print(\"added to letters\",arabic_text[ind])\n",
        "\n",
        "            if(ind+1 < len(arabic_text) and not unicodedata.combining(arabic_text[ind+1])):\n",
        "                diacritics.append(temp)\n",
        "                # print(\"added to diacritics from 1st\",temp)\n",
        "            if(ind == (len(arabic_text)-1)):\n",
        "              diacritics.append(temp)\n",
        "            ind+=1\n",
        "\n",
        "        else:\n",
        "            while ind < len(arabic_text) and unicodedata.combining(arabic_text[ind]):\n",
        "                # diacritics.pop(0)\n",
        "                # print(arabic_text[ind])\n",
        "                temp.append(arabic_text[ind])\n",
        "                ind+=1\n",
        "            temp=unicodedata.normalize('NFC', ''.join(temp))\n",
        "            # temp=[temp[::-1]]\n",
        "            diacritics.append([temp])\n",
        "            # print(\"added to diacritics\",temp)\n",
        "    # letters.reverse()\n",
        "    # diacritics.reverse()\n",
        "    return letters, diacritics\n",
        "\n",
        "########################################################################################\n",
        "def tokenize_to_vocab(data, vocab):\n",
        "    tokenized_sentences_word, tokenized_sentences_letters, tokenized_sentences_diacritics = [], [],[]\n",
        "\n",
        "    for d in (data):\n",
        "            tokens = nltk.word_tokenize(d, language=\"arabic\", preserve_line=True)\n",
        "            # Add the start sentence <s> and end sentence </s> tokens at the beginning and end of each tokenized sentence\n",
        "            tokens.reverse()\n",
        "            tokens.insert(0,\"<s>\")\n",
        "            tokens.append(\"</s>\")\n",
        "\n",
        "            vocab.update(tokens)\n",
        "\n",
        "            word_letters=[]\n",
        "            word_diacritics=[]\n",
        "            for token in (tokens):\n",
        "                if token != \"<s>\" and token != \"</s>\":\n",
        "                    # letters = separate_arabic_to_letters(token)\n",
        "                    letter, diacritic = separate_word_to_letters_diacritics(token)\n",
        "                    word_diacritics.append(diacritic)\n",
        "                    word_letters.append(letter)\n",
        "                else:\n",
        "                    word_letters.append(token)\n",
        "                    word_diacritics.append(token)\n",
        "\n",
        "            tokenized_sentences_letters.append(word_letters)\n",
        "            tokenized_sentences_diacritics.append(word_diacritics)\n",
        "            tokenized_sentences_word.append(tokens)\n",
        "\n",
        "    return vocab, tokenized_sentences_word,tokenized_sentences_letters,tokenized_sentences_diacritics\n",
        "\n",
        "def extract_sentences(training_dataset):\n",
        "    # This pattern keeps Arabic letters, diacritics, and whitespaces and endlines\n",
        "    pattern = re.compile(r'[^\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF\\s,.؟،«»؛(){};:!?\\-\\'\"]')\n",
        "\n",
        "    # Replace unmatched characters with an empty string\n",
        "    cleaned_corpus = [re.sub(pattern, \"\", t) for t in training_dataset]\n",
        "    cleaned_corpus = [re.sub(\"\\s\\s+\", \" \", c) for c in cleaned_corpus]\n",
        "\n",
        "    print(len(cleaned_corpus))\n",
        "\n",
        "    data,labels = [],[]\n",
        "\n",
        "    first = True\n",
        "    for c in cleaned_corpus:\n",
        "        sentences = re.split(r'[,.؟،«»؛(){};:!?\\-\\'\"]+', c)  # split on all punctuation\n",
        "        labels += sentences\n",
        "\n",
        "        without_dialects = [\n",
        "            strip_diacritics(s) for s in sentences\n",
        "        ]  # get the letters without dialects\n",
        "        data += without_dialects\n",
        "\n",
        "\n",
        "    # remove any spaces from line\n",
        "    data = [d.strip() for d in data]\n",
        "    labels = [l.strip() for l in labels]\n",
        "\n",
        "    # remove empty lines\n",
        "    data = [i for i in data if i]\n",
        "    labels = [i for i in labels if i]\n",
        "    return data,labels,cleaned_corpus\n",
        "\n",
        "\n",
        "########################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "5sjK73w9O5RL"
      },
      "outputs": [],
      "source": [
        "#1- Read data embeddings we have for letters and diacritics\n",
        "letters, diacritics, diacritics2id = get_letters() , get_diacritics(), get_diacritics2id()\n",
        "#2- Have mapping ready\n",
        "#### Letters ---- IDs\n",
        "letters2id = {item: index for index, item in enumerate(letters)}\n",
        "id2letters = {index: item for index, item in enumerate(letters)}\n",
        "\n",
        "# add <PAD> to the mapping\n",
        "diacritics2id['<PAD>'] = len(diacritics2id)\n",
        "diacritics2id['<s>'] = len(diacritics2id)\n",
        "diacritics2id['</s>'] = len(diacritics2id)\n",
        "id2diacritics = {value: key for key, value in diacritics2id.items()}\n",
        "for diacritic, id in diacritics2id.items():\n",
        "        if diacritic not in diacritics:\n",
        "          diacritics.add(diacritic)\n",
        "\n",
        "for letter, id in letters2id.items():\n",
        "        if letter not in letters:\n",
        "          letters.add(letter)\n",
        "# print(diacritics2id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Tls8fPgOO6-O"
      },
      "outputs": [],
      "source": [
        "# letters, diacritics, diacritics2id\n",
        "def map_data(data_raw):\n",
        "    ''' Splists data lines into an array of charachers as integers and an array of discritics as one-hot-encodings '''\n",
        "\n",
        "    # initialize data and diacritics lists\n",
        "    X = list()\n",
        "    Y = list()\n",
        "\n",
        "    # loop on data lines\n",
        "    for line in data_raw:\n",
        "\n",
        "        lit,dicc=separate_word_to_letters_diacritics(line)\n",
        "        # print(len(lit),len(dicc))\n",
        "        # for each sub list in dicc , if the len of the list is greater than 1 then take the last element only\n",
        "        # if the size is zero assert that the size is zero\n",
        "        # for i in range(len(dicc)):\n",
        "        #     if len(dicc[i])>=1:\n",
        "        #         dicc[i]=dicc[i][-1]\n",
        "        #     elif len(dicc[i])==0:\n",
        "        #         dicc[i]='<s>'\n",
        "\n",
        "        # initialize line data and diacritics lists and add start of sentence character\n",
        "        x = [letters2id['<s>']]\n",
        "        y = [diacritics2id['<s>']]\n",
        "        # x=[]\n",
        "        # y=[]\n",
        "        for i in range(min(len(lit),len(dicc))):\n",
        "            if len(dicc[i])>=1:\n",
        "                dicc[i]=dicc[i][0]\n",
        "            elif len(dicc[i])==0:\n",
        "                dicc[i]=''\n",
        "            if dicc[i] not in diacritics2id:\n",
        "                dicc[i]=''\n",
        "\n",
        "            if lit[i] not in letters2id:\n",
        "                lit[i]='<PAD>'\n",
        "                dicc[i]='<PAD>'\n",
        "\n",
        "            x.append(letters2id[lit[i]])\n",
        "            y.append(diacritics2id[dicc[i]])\n",
        "\n",
        "        # assert characters list length equals diacritics list length\n",
        "        # assert(len(x) == len(y))\n",
        "\n",
        "        # append end of sentence character\n",
        "        x.append(letters2id['</s>'])\n",
        "        y.append(diacritics2id['</s>'])\n",
        "\n",
        "        # convert diacritics integers to one_hot_encodings\n",
        "        # print(y)\n",
        "        y = to_categorical(y, len(diacritics2id))\n",
        "\n",
        "        # append line's data and diacritics lists to total data and diacritics lists\n",
        "        X.append(x)\n",
        "        Y.append(y)\n",
        "    # convert lists to numpy arrays\n",
        "    # X = np.asarray(X)\n",
        "    # Y = np.asarray(Y)\n",
        "\n",
        "    return X, Y\n",
        "class DataGenerator(Sequence):\n",
        "    ''' Costumized data generator to input line batches into the model '''\n",
        "    def __init__(self, lines, batch_size,letters2id, diacritics2id):\n",
        "        self.lines = lines\n",
        "        self.batch_size = batch_size\n",
        "        self.letters2id = letters2id\n",
        "        self.diacritics2id = diacritics2id\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.lines) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        lines = self.lines[idx * self.batch_size:(idx + 1) * self.batch_size] # load number of sentences equal to batch size\n",
        "        X_batch, Y_batch = map_data(lines) # map data to integers and one-hot-encodings\n",
        "        X_max_seq_len = np.max([len(x) for x in X_batch])\n",
        "        Y_max_seq_len = np.max([len(y) for y in Y_batch])\n",
        "        assert(X_max_seq_len == Y_max_seq_len)\n",
        "        X = list()\n",
        "        for x in X_batch:\n",
        "            x = list(x)\n",
        "            x.extend([self.letters2id['<PAD>']] * (X_max_seq_len - len(x)))\n",
        "            X.append(np.asarray(x))\n",
        "\n",
        "        Y_tmp = list()\n",
        "        for y in Y_batch:\n",
        "            y_new = list(y)\n",
        "            y_new.extend(to_categorical([self.diacritics2id['<PAD>']] * (Y_max_seq_len - len(y)), len(diacritics2id)))\n",
        "            Y_tmp.append(np.asarray(y_new))\n",
        "        Y_batch = Y_tmp\n",
        "        X_batch = np.array(X)\n",
        "        Y_batch = np.asarray(Y_batch)\n",
        "\n",
        "        return (X_batch, Y_batch)\n",
        "\n",
        "########################################################################################\n",
        "def create_model(CHARACTERS_MAPPING, CLASSES_MAPPING ):\n",
        "    ''' Creates diacritization model '''\n",
        "    SelectedLSTM = LSTM # Bidirectional Long Short-Term Memory\n",
        "\n",
        "    inputs = Input(shape=(None,))\n",
        "\n",
        "    embeddings = Embedding(input_dim=len(CHARACTERS_MAPPING),\n",
        "                           output_dim=25,\n",
        "                           embeddings_initializer=glorot_normal(seed=961))(inputs)\n",
        "\n",
        "    blstm1 = Bidirectional(SelectedLSTM(units=256,\n",
        "                                     return_sequences=True,\n",
        "                                     kernel_initializer=glorot_normal(seed=961)))(embeddings)\n",
        "    dropout1 = Dropout(0.5)(blstm1)\n",
        "    blstm2 = Bidirectional(SelectedLSTM(units=256,\n",
        "                                     return_sequences=True,\n",
        "                                     kernel_initializer=glorot_normal(seed=961)))(dropout1)\n",
        "    dropout2 = Dropout(0.5)(blstm2)\n",
        "\n",
        "    dense1 = TimeDistributed(Dense(units=512,\n",
        "                                   activation='relu',\n",
        "                                   kernel_initializer=glorot_normal(seed=961)))(dropout2)\n",
        "    dense2 = TimeDistributed(Dense(units=512,\n",
        "                                   activation='relu',\n",
        "                                   kernel_initializer=glorot_normal(seed=961)))(dense1)\n",
        "\n",
        "    output = TimeDistributed(Dense(units=len(CLASSES_MAPPING),\n",
        "                                   activation='softmax',\n",
        "                                   kernel_initializer=glorot_normal(seed=961)))(dense2)\n",
        "\n",
        "    model = Model(inputs, output)\n",
        "\n",
        "    # compile model\n",
        "    # model.compile(loss='categorical_crossentropy', optimizer=Adam())\n",
        "    # use this model to calc the accuracy\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "########################################################################################\n",
        "\n",
        "def fit_model(model, epochs, batch_size, train_split,val_split,letters2id, diacritics2id):\n",
        "    ''' Fits model '''\n",
        "\n",
        "\n",
        "    # create training and validation generators\n",
        "    training_generator = DataGenerator(train_split, batch_size,letters2id, diacritics2id)\n",
        "    val_generator = DataGenerator(val_split, batch_size,letters2id, diacritics2id)\n",
        "\n",
        "    # fit model\n",
        "    history = model.fit(x=training_generator,\n",
        "              validation_data=val_generator,\n",
        "              epochs=epochs\n",
        "              )\n",
        "\n",
        "    # return history\n",
        "    return history\n",
        "########################################################################################\n",
        "def remove_diacritics(data_raw):\n",
        "  ''' Returns undiacritized text'''\n",
        "  return data_raw.translate(str.maketrans('', '', ''.join(diacritics)))\n",
        "def predict(line, model):\n",
        "    ''' predict test line '''\n",
        "    X, _ = map_data([line])\n",
        "    # for x in X[0]:\n",
        "    #   print(id2letters[x])\n",
        "    predictions = model.predict(X,verbose=0).squeeze()\n",
        "    # print(len(remove_diacritics(line)),len(predictions))\n",
        "\n",
        "    # print(predictions[0])\n",
        "    # # get most probable diacritizations for each character\n",
        "    predictions = predictions[1:]\n",
        "    # initialize empty output line\n",
        "    output = ''\n",
        "    char_list=[]\n",
        "    diac_list=[]\n",
        "    # loop on input characters and predicted diacritizations\n",
        "    for char, prediction in zip(remove_diacritics(line), predictions):\n",
        "        if char != ' ': # ignore spaces\n",
        "            char_list.append(char)\n",
        "            diac_list.append(np.argmax(prediction))\n",
        "    # ind=0\n",
        "    # for prediction in predictions:\n",
        "    #     char=X[ind]\n",
        "        # print(char)\n",
        "        # # ind+=1\n",
        "        # print(char,id2diacritics[np.argmax(prediction)])\n",
        "        # print(np.argmax(prediction),len(prediction),len(id2diacritics))\n",
        "\n",
        "        # append character\n",
        "        output += char\n",
        "        # if character is not an arabic letter continue\n",
        "        if char not in letters:\n",
        "            continue\n",
        "\n",
        "        if '<' in id2diacritics[np.argmax(prediction)]:\n",
        "            continue\n",
        "\n",
        "        # if character in arabic letters append predicted diacritization\n",
        "        output += id2diacritics[np.argmax(prediction)]\n",
        "\n",
        "    return output,char_list,diac_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJLyVoO6O8jc",
        "outputId": "c0fd1cd7-0fca-453c-c8c8-3345b5132a6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000\n",
            "2500\n"
          ]
        }
      ],
      "source": [
        "training_dataset = read_training_dataset()\n",
        "dev_dataset = read_dev_dataset()\n",
        "data, labels, cleaned_corpus = extract_sentences(training_dataset)\n",
        "val_data, val_labels, val_cleaned_corpus = extract_sentences(dev_dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmEPqZdPf6a4",
        "outputId": "9710df91-8267-4b45-883d-02f95419dbb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 25)          975       \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, None, 512)         577536    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, 512)         0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, None, 512)         1574912   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, None, 512)         0         \n",
            "                                                                 \n",
            " time_distributed (TimeDist  (None, None, 512)         262656    \n",
            " ributed)                                                        \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDi  (None, None, 512)         262656    \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_2 (TimeDi  (None, None, 18)          9234      \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2687969 (10.25 MB)\n",
            "Trainable params: 2687969 (10.25 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = create_model(letters2id, diacritics2id)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvKIORVZf8LR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec491357-a2d9-49a8-b50a-584ec624b3bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2175/2175 [==============================] - 692s 310ms/step - loss: 0.0698 - accuracy: 0.9788 - val_loss: 0.0260 - val_accuracy: 0.9920\n",
            "--- 692.34 seconds ---\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "hist_1 = fit_model(model, 1, 128, labels, val_labels,letters2id, diacritics2id)\n",
        "end_time = time.time()\n",
        "print('--- %s seconds ---' % round(end_time - start_time, 2))\n",
        "model.save('model_epoches1')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQf-JXFrf_6_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "cecc4062-1ec9-4aea-974b-8e3d6b07f7dd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(hist_1.history['loss'])\n",
        "plt.plot(hist_1.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('model_epoches1')\n"
      ],
      "metadata": {
        "id": "3Wg1DIEEQXgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOEy2aoHgCAj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "481e3e10-7d90-4cc5-ce55-d64f371d80a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2500\n",
            "14085/14085 [==============================] - 98s 7ms/step - loss: 7.6097 - accuracy: 0.1611\n",
            "accracy in the test set is :  0.16110213100910187\n"
          ]
        }
      ],
      "source": [
        "test_data = read_dev_dataset('dataset/test.txt')\n",
        "test_data,test_labels,test_corpus = extract_sentences(test_data)\n",
        "data_gen = DataGenerator(test_labels, 1,letters2id, diacritics2id)\n",
        "acc = model.evaluate(data_gen)[1]\n",
        "print(\"accracy in the test set is : \",acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNAyo1eigExH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b90a401a-b356-4805-8dd2-d0660140d8a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ذهبْ زْيُاد الَيَ الَمَدِرَسَةِ\n",
            "الِعِصَفوَر فوَق الِشَجرَةِ الكبَيَرَةَ\n",
            "احَمدِ يَحَبَ حَنِانَ\n",
            "الَولْدِ يلْعَبْ تَحَتَ الِشُجرَه\n"
          ]
        }
      ],
      "source": [
        "print(predict('ذهب زياد الي المدرسة', model)[0])\n",
        "print(predict(\"العصفور فوق الشجرة الكبيرة\", model)[0])\n",
        "print(predict(\"احمد يحب حنان\", model)[0])\n",
        "print(predict(\"الولد يلعب تحت الشجره\", model)[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import time\n",
        "test_data = read_dev_dataset('./test.txt')\n",
        "data, labels, cleaned_corpus = extract_sentences(test_data)\n",
        "\n",
        "last_id=0\n",
        "# open csv fiel as write\n",
        "file = open('result.csv', 'w', newline='', encoding='utf-8')\n",
        "writer = csv.writer(file)\n",
        "writer.writerow(['ID', 'label'])\n",
        "\n",
        "\n",
        "# out_list =[]\n",
        "\n",
        "start_time = time.time()\n",
        "for i in range(len(data)):\n",
        "    out , char_list, diac_list = predict(data[i], model)\n",
        "    for j in range(len(char_list)):\n",
        "        if char_list[j] == ' ':\n",
        "            continue\n",
        "        writer.writerow([last_id, diac_list[j]])\n",
        "        last_id+=1\n",
        "\n",
        "file.close()\n",
        "\n",
        "end_time = time.time()\n",
        "print('--- %s seconds ---' % round(end_time - start_time, 2))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwL5jt_DjkBU",
        "outputId": "a5311e8b-fdf2-4769-ffe5-15368b391fff"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8351478\n",
            "50000\n",
            "8351478\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the datasets\n",
        "gold_data = pd.read_csv('sample_test_set_gold.csv')\n",
        "predicted_data = pd.read_csv('result.csv')  # Assuming this contains your predictions\n",
        "\n",
        "# Ensure both dataframes have the same length\n",
        "assert len(gold_data) == len(predicted_data), \"Datasets must have the same number of rows\"\n",
        "\n",
        "# Compare and calculate accuracy\n",
        "correct_predictions = 0\n",
        "for gold_label, predicted_label in zip(gold_data['label'], predicted_data['label']):\n",
        "    if gold_label == predicted_label:\n",
        "        correct_predictions += 1\n",
        "\n",
        "accuracy = correct_predictions / (len(gold_data)-1)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "JYOf7pbAjpwZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}